# Numpy Library for Numerical Calculations
import numpy as np

# Pandas Library for Dataframe
import pandas as pd

# Matplotlib and for Plottings
import matplotlib.pyplot as plt

# Train_Test_Split for splitting the Dataset
from sklearn.model_selection import train_test_split

# Logistic Regression, Random Forest Classifier and SVC are Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Accuracy Score and Confusion Matrix is for Analysis of Models
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

# Reading Information in the Data Set
from google.colab import drive
drive.mount('/content/drive')
spam = pd.read_csv('/content/drive/My Drive/emails.csv')

# Check for null values in Data
spam.isnull().sum()
spam.dropna(inplace=True)

# Checking the Shape of Data
print(spam.shape)
print(spam.info())
print(spam.describe)

# Count the number of spam and non-spam emails
spam_count = sum(spam['spam'])
non_spam_count = len(spam) - spam_count
total_emails = len(spam)

# Print the count of spam and non-spam emails
print("\nNumber of spam emails:", spam_count)
print("Number of non-spam emails:", non_spam_count)
print("Total number of emails:", total_emails)

# Selecting feautures and label
X = spam.iloc[:,1:3001]
print(X)

y = spam['Prediction']
print(y)

train_X , test_X, train_y , test_y = train_test_split(X,y,test_size=0.2)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
train_X = sc.fit_transform(train_X)
test_X = sc.transform(test_X)

# Train and Test Data using the Logistic Regresion Model
lr = LogisticRegression(solver='lbfgs', max_iter=1000)
lr.fit(train_X,train_y)
c = lr.intercept_
print("c=", c)
m =lr.coef_
print("m=", m)

y_pred = lr.predict(test_X)
print("Accuracy Score for LR : ", accuracy_score(test_y,y_pred))
print("Confusion Matrix for LR : \n", confusion_matrix(test_y,y_pred))
print("Precision Score for LR : ", precision_score(test_y,y_pred))
print("Recall Score for LR : ", recall_score(test_y,y_pred))
print("F1 Score for LR : ",f1_score(test_y,y_pred))
print("ROC_AUC_Score for LR : ", roc_auc_score(test_y, lr.predict_proba(test_X)[:, 1]))

# Train and Test Data using SVC Model
svc = SVC(C=1.0, kernel='rbf', gamma='auto', probability=True)
svc.fit(train_X, train_y)
y_pred1 = svc.predict(test_X)
print("Accuracy Score for SVC : ", accuracy_score(test_y, y_pred1))
print("Confusion Matrix for SVC : \n", confusion_matrix(test_y, y_pred1))
print("Precision Score for SVC : ", precision_score(test_y, y_pred1))
print("Recall Score for SVC : ", recall_score(test_y, y_pred1))
print("F1 Score for SVC : ",f1_score(test_y, y_pred1))
print("ROC_AUC_Score for SVC : ", roc_auc_score(test_y, svc.predict_proba(test_X)[:, 1]))

# Train and Test Data using Random Forest Classifier Model
rfc = RandomForestClassifier(n_estimators=1000,criterion='entropy')
# n_estimators = No. of trees in the forest
# criterion = basis of making the decision tree split, either on gini impurity('gini'), or on infromation gain('entropy')
rfc.fit(train_X,train_y)
y_pred2 = rfc.predict(test_X)
print("Accuracy Score of Random Forest Classifier : ", accuracy_score(y_pred2,test_y))
print("Confusion Matrix for RFC : \n", confusion_matrix(test_y, y_pred2))
print("Precision Score for RFC : ", precision_score(test_y, y_pred2))
print("Recall Score for RFC : ", recall_score(test_y, y_pred2))
print("F1 Score for RFC : ",f1_score(test_y, y_pred2))
print("ROC_AUC_Score for RFC : ", roc_auc_score(test_y, rfc.predict_proba(test_X)[:, 1]))

# MODEL COMPARISON
accuracy = [accuracy_score(test_y, y_pred),accuracy_score(test_y,y_pred1), accuracy_score(test_y, y_pred2)]
f1_score = [f1_score(test_y, y_pred,) ,f1_score(test_y,y_pred1),f1_score(test_y, y_pred2)]
recall = [recall_score(test_y, y_pred), recall_score(test_y,y_pred1), recall_score(test_y, y_pred2)]
roc_auc = [roc_auc_score(test_y, lr.predict_proba(test_X)[:, 1]), roc_auc_score(test_y, svc.predict_proba(test_X)[:, 1]), roc_auc_score(test_y, rfc.predict_proba(test_X)[:, 1])]

models = ["Logistic Regression","Support Vector Classification","Random Forest Classification"]
plt.figure(figsize=(10,5))
plt.bar(models, accuracy)
plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Accuracy of the Models")
plt.show()

models = [LogisticRegression(max_iter=1000), SVC(C=1.0, kernel='rbf', gamma='auto', probability=True), RandomForestClassifier(n_estimators=1000,criterion='entropy')]
def compare_model_train_test():
  for model in models:
    model.fit(train_X, train_y)
    test_data_prediction = model.predict(test_X)
    accuracy = accuracy_score(test_y, test_data_prediction)
    print('Accuracy score of the',model,'=', accuracy)

compare_model_train_test()

#CROSS VALIDATION
from sklearn.model_selection import cross_val_score

cv_score_lr = cross_val_score(LogisticRegression(max_iter=1000),X, y, cv=5)
print(cv_score_lr)

mean_accuracy_lr = sum(cv_score_lr)/len(cv_score_lr)
mean_accuracy_lr = mean_accuracy_lr*100
mean_accuracy_lr = round(mean_accuracy_lr, 2)
print(mean_accuracy_lr )

cv_score_svc = cross_val_score(SVC(kernel='linear'),X, y, cv=5)
print(cv_score_svc)

mean_accuracy_svc = sum(cv_score_svc)/len(cv_score_svc)
mean_accuracy_svc = mean_accuracy_svc*100
mean_accuracy_svc = round(mean_accuracy_svc, 2)
print(mean_accuracy_svc )

cv_score_rfc = cross_val_score(rfc, X,y,cv=5)
print(cv_score_rfc)

mean_accuracy_rfc = sum(cv_score_rfc)/len(cv_score_rfc)
mean_accuracy_rfc = mean_accuracy_rfc*100
mean_accuracy_rfc = round(mean_accuracy_rfc, 2)
print(mean_accuracy_rfc )
